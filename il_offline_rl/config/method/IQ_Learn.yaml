# @package _global_




method:
  method_name: 'IQ_Learn'
  batch_size: 256
  max_timesteps: 5e5
  absorbing: False
  norm_obs: False
  use_target: True
  loss_type: 'v0' # ['v0', 'value']
  grad_pen: [False, 0.]
  regularize: [True, 0.5]
  action_gap_reg: [False, 0.]
  num_random_steps: 0
  start_training_steps: 1280





agent:
  hidden_size: 256
  policy_dist: 'TNorm'
  is_V_critic: False
  is_mlp_base: ${env.is_not_Atari}

  is_double_Q: False
  actor_lr: 3e-5
  critic_lr: 3e-4
  alpha_lr: 3e-4

  critic_tau: 0.005 # critic soft update coff
  target_critic_update_freq: 1 # 1000 for Atari tasks
  learnable_temperature: True # no use till now
  init_temperature: 0.01